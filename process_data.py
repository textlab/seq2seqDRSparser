import json
import os
import subprocess
import sys

with open("config.json","r") as f:
    config=json.load(f)

datasets=config["datasets"]

def clear_file(filename):
    f=open(filename,"r")
    content=f.read()
    f.close()
    f=open(filename,"w")
    content=content.split('%%% This output was generated by the following command:')
    content=content[1:]
    for box in content:
        box=box.split('\n')
        for line in box:
            line = line.split('%')
            if(line[0]!=""):
                f.write(line[0].strip())
                f.write('\n')
        f.write('\n')
    f.close()


# Write anything given as paramter to STDERR
def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

def run_this(exe):
    print("Running ", exe)
    proc = subprocess.Popen(exe , stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
    res = proc.communicate()
    print(res[0].decode())
    eprint(res[1].decode())
    
    #print(out)



# Merges the input_files into output_file
def merge_files(input_files, output_file):
    with open(output_file, 'w') as outfile:
        for names in input_files:
            with open(names) as infile:
                outfile.write(infile.read())

# Truncates the file name given as parameter
def truncate_file(file_name):
    f=open(file_name,"w")
    f.truncate()
    f.close()

# Creates the directory of a file
def create_path(file_name):
    head, tail = os.path.split(file_name)
    os.makedirs(head, exist_ok=True)

# Appends content of file1 to file2
def append_content(file1, file2):
    with open(file2, 'a') as outfile:
        with open(file1) as infile:
            outfile.write(infile.read())

# Creates serialized version of relative notation file
def create_serialized_relative_notat_file(inp_name, out_name):
    content = ""
    with open(inp_name,"r") as infile:
        content=infile.read()
    content = content.split("\n\n")
     
    with open(out_name, "w") as outfile:
        whole_content=""
        for drs in content:
            drs = drs.strip()
            if drs == "":
                continue
            drs = drs.replace("\n", " | ")
            whole_content += "| " + drs+"\n"
        outfile.write(whole_content[:-1])

# Removes the last newline character from the file
def remove_last_newline(f_name):
    content =""
    with open(f_name,"r") as f:
        content = f.read()
    content = content.strip()
    with open(f_name,"w") as f:
        f.write(content)

# The main script starts from here
ml_dataset_out_main=config["ml_dataset_out_dir"]

# For each ml_dataset
for ml_dataset_name in datasets:
        if "no_operation" in datasets[ml_dataset_name] and datasets[ml_dataset_name]["no_operation"]=="True":
            print("No operation for " + ml_dataset_name)
            continue
        if "process" in datasets[ml_dataset_name] and datasets[ml_dataset_name]["process"]=="False":
            ml_dataset_out_dir = ml_dataset_out_main + "/" + ml_dataset_name + "/"
            print("Processing disabled for " + ml_dataset_name + ". Assuming the dataset is already processed. So, trying to copy the processed files.")
            print("Creating " + ml_dataset_name + " dataset.")
            copy_from=datasets[ml_dataset_name]["copy_from"]
            copy_to=datasets[ml_dataset_name]["copy_to"]
            if len(copy_from) != len(copy_to):
                print("Length of copy_from and copy_to are not equal. Aborting for any action for "+ ml_dataset_name)
                continue

            for file_name in copy_to:
                file_name = ml_dataset_out_dir + file_name
                print("Truncating file: "+ file_name)
                create_path(file_name)
                truncate_file(file_name)

            for i in range(len(copy_from)):
                print("Appending the content of " + copy_from[i] + " to "+ ml_dataset_out_dir + copy_to[i]+".")
                append_content(copy_from[i], ml_dataset_out_dir + copy_to[i])

            if datasets[ml_dataset_name]["create_serialized_relative_notation_file"]=="True":
                inp_name = ml_dataset_out_dir + datasets[ml_dataset_name]["train_output_var_file"]
                out_name = ml_dataset_out_dir + datasets[ml_dataset_name]["serialized_relative_notation_file_save"]
                head, tail = os.path.split(out_name)
                print("Creating serialized relative notation file: " + out_name)
                os.makedirs(head, exist_ok=True)
                create_serialized_relative_notat_file(inp_name, out_name)
                # Remove trailing newline so that the line numbers will be equal for both input and output files
                remove_last_newline(ml_dataset_out_dir + datasets[ml_dataset_name]["train_input_file_save"])
            continue
        print("Creating " + ml_dataset_name + " dataset.")
        ml_dataset_out_dir = ml_dataset_out_main + "/" + ml_dataset_name + "/"
        train_input_files=datasets[ml_dataset_name]["train_input_files"]
        test_input_files=datasets[ml_dataset_name]["test_input_files"]
        train_output_files=datasets[ml_dataset_name]["train_output_files"]
        test_output_files=datasets[ml_dataset_name]["test_output_files"]

        train_input_file_save=ml_dataset_out_dir + datasets[ml_dataset_name]["train_input_file_save"]
        train_output_file_save=ml_dataset_out_dir + datasets[ml_dataset_name]["train_output_file_save"]
        test_input_file_save=ml_dataset_out_dir + datasets[ml_dataset_name]["test_input_file_save"]
        test_output_file_save=ml_dataset_out_dir + datasets[ml_dataset_name]["test_output_file_save"]
        train_output_var_files_input=datasets[ml_dataset_name]["train_output_var_files_input"]
        train_output_var_file_save=ml_dataset_out_dir + datasets[ml_dataset_name]["train_output_var_file"]
        eval_input_files=datasets[ml_dataset_name]["eval_input_files"]
        dev_input_files=datasets[ml_dataset_name]["dev_input_files"]
        eval_output_files=datasets[ml_dataset_name]["eval_output_files"]
        dev_output_files=datasets[ml_dataset_name]["dev_output_files"]

        dev_output_file_save=ml_dataset_out_dir + datasets[ml_dataset_name]["dev_output_file_save"]
        dev_input_file_save=ml_dataset_out_dir + datasets[ml_dataset_name]["dev_input_file_save"]
        eval_output_file_save=ml_dataset_out_dir + datasets[ml_dataset_name]["eval_output_file_save"]
        eval_input_file_save=ml_dataset_out_dir + datasets[ml_dataset_name]["eval_input_file_save"]


        # Create output directories
        head, tail = os.path.split(train_input_file_save)
        os.makedirs(head, exist_ok=True)
        head, tail = os.path.split(train_output_file_save)
        os.makedirs(head, exist_ok=True)
        head, tail = os.path.split(test_input_file_save)
        os.makedirs(head, exist_ok=True)
        head, tail = os.path.split(test_output_file_save)
        os.makedirs(head, exist_ok=True)
        head, tail = os.path.split(dev_input_file_save)
        os.makedirs(head, exist_ok=True)
        head, tail = os.path.split(dev_output_file_save)
        os.makedirs(head, exist_ok=True)

        
        if len(train_output_var_files_input)>0:
            head, tail = os.path.split(train_output_var_file_save)
            os.makedirs(head, exist_ok=True)

        if len(eval_input_files)>0:
            head, tail = os.path.split(eval_output_file_save)
            os.makedirs(head, exist_ok=True)


        # Check if preprocessing required
        executable=""
        if datasets[ml_dataset_name]["preprocess_train_output"]=="True":
            executable=datasets[ml_dataset_name]["preprocess_exec"]

        # Create the files accordingly
        merge_files(train_output_files,train_output_file_save)
        merge_files(test_input_files,test_input_file_save)
        merge_files(train_input_files,train_input_file_save)
        merge_files(test_output_files,test_output_file_save)
        merge_files(dev_output_files,dev_output_file_save)
        merge_files(dev_input_files,dev_input_file_save)

        if len(train_output_var_files_input)>0:
            merge_files(train_output_var_files_input,train_output_var_file_save)

        if len(eval_input_files)>0:
            merge_files(eval_output_files,eval_output_file_save)
            merge_files(eval_input_files,eval_input_file_save)

        # Clear the training output file from comments (to be used in tokenizer training)
        clear_file(train_output_file_save)

        # Create relative clauses
        if executable!="":
            head, tail = os.path.split(train_output_var_file_save)
            os.makedirs(head, exist_ok=True)
            print("Preprocess executable will run")
            run_this(executable.replace("{dev_output_file_save}", dev_output_file_save).replace("{dev_input_file_save}",dev_input_file_save).replace("{eval_output_file_save}",eval_output_file_save).replace("{eval_input_file_save}", eval_input_file_save).replace("{test_output_file_save}", test_output_file_save).replace("{test_input_file_save}",test_input_file_save).replace("{train_output_file_save}",train_output_file_save).replace("{train_input_file_save}",train_input_file_save))
        if datasets[ml_dataset_name]["create_serialized_relative_notation_file"]=="True":
            inp_name = ml_dataset_out_dir + datasets[ml_dataset_name]["train_output_var_file"]
            out_name = ml_dataset_out_dir + datasets[ml_dataset_name]["serialized_relative_notation_file_save"]
            head, tail = os.path.split(out_name)
            print("Creating serialized relative notation file: " + out_name)
            os.makedirs(head, exist_ok=True)
            create_serialized_relative_notat_file(inp_name, out_name)

