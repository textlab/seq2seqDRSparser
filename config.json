{
"ml_dataset_out_dir":"data/ml",
"tokenizer_out_dir":"tokenizers",
"datasets":{
	"pmb220_en_org":{
		 	"no_operation":"False",
			"test_input_files":["data/source/exp_data_2.2.0/en/gold/test.txt.raw"],
			"train_input_files":
                              ["data/source/exp_data_2.2.0/en/bronze/train.txt.raw",
                               "data/source/exp_data_2.2.0/en/silver/train.txt.raw",
                                "data/source/exp_data_2.2.0/en/gold/train.txt.raw"],
			"dev_input_files":["data/source/exp_data_2.2.0/en/gold/dev.txt.raw"],
			"eval_input_files":[],
			"train_output_files":
			      ["data/source/exp_data_2.2.0/en/bronze/train.txt",
				"data/source/exp_data_2.2.0/en/silver/train.txt",
				"data/source/exp_data_2.2.0/en/gold/train.txt"],
			"test_output_files":["data/source/exp_data_2.2.0/en/gold/test.txt"],
			"dev_output_files":["data/source/exp_data_2.2.0/en/gold/dev.txt"],
			"eval_output_files":[],
			"train_output_var_files_input":[],
			"train_output_var_file":"train_out.txt.var",
			"train_input_file_save":"train_in.txt",
			"train_output_file_save":"train_out.txt",
                        "test_input_file_save":"test_in.txt",
                        "test_output_file_save":"test_out.txt",
			"dev_output_file_save":"dev_out.txt",
			"dev_input_file_save":"dev_in.txt",
                        "eval_output_file_save":"",
                        "eval_input_file_save":"",
			"preprocess_train_output":"True",
			"preprocess_exec":"other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/preprocess.py -i {dev_output_file_save} -s {dev_input_file_save} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/preprocess.py -i {test_output_file_save} -s {test_input_file_save} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/preprocess.py -i {train_output_file_save} -s {train_input_file_save} -sig other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -ri && cp {train_input_file_save}.fil {train_input_file_save}",
			"create_serialized_relative_notation_file":"True",
			"serialized_relative_notation_file_save":"train_out.txt.ser"
	},
        "pmb300_en_org":{
			"no_operation":"False",
                        "test_input_files":["data/source/exp_data_3.0.0/en/gold/test.txt.raw"],
                        "train_input_files":
                              ["data/source/exp_data_3.0.0/en/bronze/train.txt.raw",
                               "data/source/exp_data_3.0.0/en/silver/train.txt.raw",
                                "data/source/exp_data_3.0.0/en/gold/train.txt.raw"],
                        "dev_input_files":["data/source/exp_data_3.0.0/en/gold/dev.txt.raw"],
                        "eval_input_files":[],
                        "train_output_files":
                              ["data/source/exp_data_3.0.0/en/bronze/train.txt",
                                "data/source/exp_data_3.0.0/en/silver/train.txt",
                                "data/source/exp_data_3.0.0/en/gold/train.txt"],
                        "test_output_files":["data/source/exp_data_3.0.0/en/gold/test.txt"],
                        "dev_output_files":["data/source/exp_data_3.0.0/en/gold/dev.txt"],
                        "eval_output_files":[],
                        "train_output_var_files_input":[],
                        "train_output_var_file":"train_out.txt.var",
                        "train_input_file_save":"train_in.txt",
                        "train_output_file_save":"train_out.txt",
                        "test_input_file_save":"test_in.txt",
                        "test_output_file_save":"test_out.txt",
                        "dev_output_file_save":"dev_out.txt",
                        "dev_input_file_save":"dev_in.txt",
                        "eval_output_file_save":"",
                        "eval_input_file_save":"",
                        "preprocess_train_output":"True",
                        "preprocess_exec":"other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/preprocess.py -i {dev_output_file_save} -s {dev_input_file_save} && other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/preprocess.py -i {test_output_file_save} -s {test_input_file_save} && other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/preprocess.py -i {train_output_file_save} -s {train_input_file_save} -sig other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -ri -e {train_input_file_save} && cp {train_input_file_save}.fil {train_input_file_save}",
                        "postprocess_exec": "",
                        "create_serialized_relative_notation_file":"True",
                        "serialized_relative_notation_file_save":"train_out.txt.ser"

        },
        "pmb400_en_org":{
			"no_operation": "False",
			"process":"True",
			"test_input_files":["data/source/exp_data_4.0.0/en/gold/test.txt.raw"],
                        "train_input_files":
                              ["data/source/exp_data_4.0.0/en/bronze/train.txt.raw",
                               "data/source/exp_data_4.0.0/en/silver/train.txt.raw",
                                "data/source/exp_data_4.0.0/en/gold/train.txt.raw"],
                        "dev_input_files":["data/source/exp_data_4.0.0/en/gold/dev.txt.raw"],
                        "eval_input_files":["data/source/exp_data_4.0.0/en/gold/eval.txt.raw"],
                        "train_output_files":
                              ["data/source/exp_data_4.0.0/en/bronze/train.txt",
                                "data/source/exp_data_4.0.0/en/silver/train.txt",
                                "data/source/exp_data_4.0.0/en/gold/train.txt"],
                        "test_output_files":["data/source/exp_data_4.0.0/en/gold/test.txt"],
                        "dev_output_files":["data/source/exp_data_4.0.0/en/gold/dev.txt"],
                        "eval_output_files":["data/source/exp_data_4.0.0/en/gold/eval.txt"],
                        "train_output_var_files_input":[],
                        "train_output_var_file":"train_out.txt.var",
                        "train_input_file_save":"train_in.txt",
                        "train_output_file_save":"train_out.txt",
                        "test_input_file_save":"test_in.txt",
                        "test_output_file_save":"test_out.txt",
                        "dev_output_file_save":"dev_out.txt",
                        "dev_input_file_save":"dev_in.txt",
                        "eval_output_file_save":"eval_out.txt",
                        "eval_input_file_save":"eval_in.txt",
                        "preprocess_train_output":"True",
                        "preprocess_exec":"other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/preprocess.py -i {dev_output_file_save} -s {dev_input_file_save} && other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/preprocess.py -i {test_output_file_save} -s {test_input_file_save} && other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/preprocess.py -i {train_output_file_save} -s {train_input_file_save} -sig other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -ri -e {train_input_file_save} && cp {train_input_file_save}.fil {train_input_file_save}",
                        "postprocess_exec": "",
                        "create_serialized_relative_notation_file":"True",
                        "serialized_relative_notation_file_save":"train_out.txt.ser"

        }
},
"tokenizers":{
	"pmb220_en_25000_3_2500_3_enc":{
		"comment":"This tokenizer is based on BertWordPieceTokenizer. It uses pmb220, english train dataset. It uses lowercase=true.",
		"no_operation": "False",
		"vocab_size": 25000,
		"min_freq": 3,
		"max_length":512,
		"lowercase" : "True",
                "tokenizer_trainer_dataset":"pmb220_en_org",
                "tokenizer_trainer_file" : "train_in.txt.fil",
		"special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
		"out_dir":"pmb220_en_25000_3_2500_3/enc",
		"training_script":["from tokenizers import BertWordPieceTokenizer",
				   "from tokenizers.processors import BertProcessing",
				   "global tokenizer",
				   "tokenizer=BertWordPieceTokenizer(lowercase=lowercase)",
				   "tokenizer.do_lower_case = lowercase",
				   "tokenizer.train(files=[filename], vocab_size=vocabsize, min_frequency=min_freq, special_tokens = special_tokens)",
				   "tokenizer._tokenizer.post_processor = BertProcessing(('[SEP]', tokenizer.token_to_id('[SEP]')), ('[CLS]', tokenizer.token_to_id('[CLS]')),)",
				   "tokenizer.enable_truncation(max_length=max_length)"],
		"loading_script":["from transformers import BertTokenizerFast",
			"global tokenizer",
			"tokenizer=BertTokenizerFast(os.path.join({out_dir}, 'vocab.txt'),do_lower_case={lowercase})"]

	},
        "pmb220_en_25000_3_2500_3_dec":{
		"comment":"This tokenizer is based on BertWordPieceTokenizer. It uses pmb220, english train dataset for decoder. It uses lowercase=false.",
		"no_operation": "False",
                "vocab_size": 25000,
                "min_freq": 3,
                "max_length":512,
                "lowercase": "False",
		"tokenizer_trainer_dataset":"pmb220_en_org",
                "tokenizer_trainer_file" : "train_out.txt.ser",
		"special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
		"out_dir":"pmb220_en_25000_3_2500_3/dec",
                "training_script":["from tokenizers import BertWordPieceTokenizer",
                                   "from tokenizers.processors import BertProcessing",
				   "global tokenizer",
                                   "tokenizer=BertWordPieceTokenizer(lowercase=lowercase)",
                                   "tokenizer.do_lower_case = lowercase",
                                   "tokenizer.train(files=[filename], vocab_size=vocabsize, min_frequency=min_freq, special_tokens = special_tokens)",
                                   "tokenizer._tokenizer.post_processor = BertProcessing(('[SEP]', tokenizer.token_to_id('[SEP]')), ('[CLS]', tokenizer.token_to_id('[CLS]')),)",
                                   "tokenizer.enable_truncation(max_length=max_length)"],
                "loading_script":["from transformers import BertTokenizerFast",
                        "global tokenizer",
                        "tokenizer=BertTokenizerFast(os.path.join({out_dir}, 'vocab.txt'),do_lower_case={lowercase})"]
        },
	"pmb300_en_25000_3_2500_3_enc":{
		"comment":"This tokenizer is based on BertWordPieceTokenizer. It uses pmb300, english train dataset. It uses lowercase=true.",
		"no_operation": "False",
		"vocab_size": 25000,
		"min_freq": 3,
		"max_length":512,
		"lowercase" : "True",
                "tokenizer_trainer_dataset":"pmb300_en_org",
                "tokenizer_trainer_file" : "train_in.txt.fil",
		"special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
		"out_dir":"pmb300_en_25000_3_2500_3/enc",
		"training_script":["from tokenizers import BertWordPieceTokenizer",
				   "from tokenizers.processors import BertProcessing",
				   "global tokenizer",
				   "tokenizer=BertWordPieceTokenizer(lowercase=lowercase)",
				   "tokenizer.do_lower_case = lowercase",
				   "tokenizer.train(files=[filename], vocab_size=vocabsize, min_frequency=min_freq, special_tokens = special_tokens)",
				   "tokenizer._tokenizer.post_processor = BertProcessing(('[SEP]', tokenizer.token_to_id('[SEP]')), ('[CLS]', tokenizer.token_to_id('[CLS]')),)",
				   "tokenizer.enable_truncation(max_length=max_length)"],
		"loading_script":["from transformers import BertTokenizerFast",
			"global tokenizer",
			"tokenizer=BertTokenizerFast(os.path.join({out_dir}, 'vocab.txt'),do_lower_case={lowercase})"]

	},
        "pmb300_en_25000_3_2500_3_dec":{
		"comment":"This tokenizer is based on BertWordPieceTokenizer. It uses pmb300, english train dataset for decoder. It uses lowercase=false.",
		"no_operation": "False",
                "vocab_size": 25000,
                "min_freq": 3,
                "max_length":512,
                "lowercase": "False",
		"tokenizer_trainer_dataset":"pmb300_en_org",
                "tokenizer_trainer_file" : "train_out.txt.ser",
		"special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
		"out_dir":"pmb300_en_25000_3_2500_3/dec",
                "training_script":["from tokenizers import BertWordPieceTokenizer",
                                   "from tokenizers.processors import BertProcessing",
				   "global tokenizer",
                                   "tokenizer=BertWordPieceTokenizer(lowercase=lowercase)",
                                   "tokenizer.do_lower_case = lowercase",
                                   "tokenizer.train(files=[filename], vocab_size=vocabsize, min_frequency=min_freq, special_tokens = special_tokens)",
                                   "tokenizer._tokenizer.post_processor = BertProcessing(('[SEP]', tokenizer.token_to_id('[SEP]')), ('[CLS]', tokenizer.token_to_id('[CLS]')),)",
                                   "tokenizer.enable_truncation(max_length=max_length)"],
                "loading_script":["from transformers import BertTokenizerFast",
                        "global tokenizer",
                        "tokenizer=BertTokenizerFast(os.path.join({out_dir}, 'vocab.txt'),do_lower_case={lowercase})"]
        },
	"pmb400_en_25000_3_2500_3_enc":{
		"comment":"This tokenizer is based on BertWordPieceTokenizer. It uses pmb400, english train dataset. It uses lowercase=true.",
		"no_operation": "False",
		"vocab_size": 25000,
		"min_freq": 3,
		"max_length":512,
		"lowercase" : "True",
                "tokenizer_trainer_dataset":"pmb400_en_org",
                "tokenizer_trainer_file" : "train_in.txt.fil",
		"special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
		"out_dir":"pmb400_en_25000_3_2500_3/enc",
		"training_script":["from tokenizers import BertWordPieceTokenizer",
				   "from tokenizers.processors import BertProcessing",
				   "global tokenizer",
				   "tokenizer=BertWordPieceTokenizer(lowercase=lowercase)",
				   "tokenizer.do_lower_case = lowercase",
				   "tokenizer.train(files=[filename], vocab_size=vocabsize, min_frequency=min_freq, special_tokens = special_tokens)",
				   "tokenizer._tokenizer.post_processor = BertProcessing(('[SEP]', tokenizer.token_to_id('[SEP]')), ('[CLS]', tokenizer.token_to_id('[CLS]')),)",
				   "tokenizer.enable_truncation(max_length=max_length)"],
		"loading_script":["from transformers import BertTokenizerFast",
			"global tokenizer",
			"tokenizer=BertTokenizerFast(os.path.join({out_dir}, 'vocab.txt'),do_lower_case={lowercase})"]

	},
        "pmb400_en_25000_3_2500_3_dec":{
		"comment":"This tokenizer is based on BertWordPieceTokenizer. It uses pmb400, english train dataset for decoder. It uses lowercase=false.",
		"no_operation": "False",
                "vocab_size": 25000,
                "min_freq": 3,
                "max_length":512,
                "lowercase": "False",
		"tokenizer_trainer_dataset":"pmb400_en_org",
                "tokenizer_trainer_file" : "train_out.txt.ser",
		"special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
		"out_dir":"pmb400_en_25000_3_2500_3/dec",
                "training_script":["from tokenizers import BertWordPieceTokenizer",
                                   "from tokenizers.processors import BertProcessing",
				   "global tokenizer",
                                   "tokenizer=BertWordPieceTokenizer(lowercase=lowercase)",
                                   "tokenizer.do_lower_case = lowercase",
                                   "tokenizer.train(files=[filename], vocab_size=vocabsize, min_frequency=min_freq, special_tokens = special_tokens)",
                                   "tokenizer._tokenizer.post_processor = BertProcessing(('[SEP]', tokenizer.token_to_id('[SEP]')), ('[CLS]', tokenizer.token_to_id('[CLS]')),)",
                                   "tokenizer.enable_truncation(max_length=max_length)"],
                "loading_script":["from transformers import BertTokenizerFast",
                        "global tokenizer",
                        "tokenizer=BertTokenizerFast(os.path.join({out_dir}, 'vocab.txt'),do_lower_case={lowercase})"]
        },
        "bert_base_uncased":{
                "comment":"This is bert_base_uncased from HuggingFace. It is an already trained tokenizer, so no need to train.",
                "no_operation": "True",
                "vocab_size": 30521,
                "min_freq": 3,
                "max_length":512,
                "lowercase" : "True",
                "tokenizer_trainer_dataset":"",
                "tokenizer_trainer_file" : "",
                "special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
                "out_dir":"",
                "training_script":[],
                "loading_script":["from transformers import BertTokenizerFast",
                        "global tokenizer",
			"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"]
        },
        "bert_base_cased":{
                "comment":"This is bert_base_cased from HuggingFace. It is an already trained tokenizer, so no need to train.",
                "no_operation": "True",
                "vocab_size": 28996,
                "min_freq": 3,
                "max_length":512,
                "lowercase" : "False",
                "tokenizer_trainer_dataset":"",
                "tokenizer_trainer_file" : "",
                "special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
                "out_dir":"",
                "training_script":[],
                "loading_script":["from transformers import BertTokenizerFast",
                        "global tokenizer",
                        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"]
        },
        "bert_large_cased":{
                "comment":"This is bert_large_cased from HuggingFace. It is an already trained tokenizer, so no need to train.",
                "no_operation": "True",
                "vocab_size": 28996,
                "min_freq": 3,
                "max_length":512,
                "lowercase" : "False",
                "tokenizer_trainer_dataset":"",
                "tokenizer_trainer_file" : "",
                "special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
                "out_dir":"",
                "training_script":[],
                "loading_script":["from transformers import BertTokenizerFast",
                        "global tokenizer",
                        "tokenizer = BertTokenizerFast.from_pretrained('bert-large-cased')"]
        },
        "bert_large_uncased":{
                "comment":"This is bert_large_uncased from HuggingFace. It is an already trained tokenizer, so no need to train.",
                "no_operation": "True",
                "vocab_size": 30521,
                "min_freq": 3,
                "max_length":512,
                "lowercase" : "True",
                "tokenizer_trainer_dataset":"",
                "tokenizer_trainer_file" : "",
                "special_tokens":["[S]","[PAD]","[/S]","[UNK]","[MASK]", "[SEP]","[CLS]"],
                "out_dir":"",
                "training_script":[],
                "loading_script":["from transformers import BertTokenizerFast",
                        "global tokenizer",
                        "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased')"]
        }
},
"models":{
	"pmb220_No-PT_8x512-8_8x512-8":{
		"comment":"This is English seq2seq model trained on pmb220 English train data",
		"do_train":"True",
		"enc_tokenizer":"pmb220_en_25000_3_2500_3_enc",
		"dec_tokenizer": "pmb220_en_25000_3_2500_3_dec",
		"train_input_file": "data/ml/pmb220_en_org/train_in.txt",
		"train_output_file": "data/ml/pmb220_en_org/train_out.txt.ser",
		"test_input_file": "data/ml/pmb220_en_org/test_in.txt",
		"test_output_file": "data/ml/pmb220_en_org/test_out.txt",
		"dev_input_file": "data/ml/pmb220_en_org/dev_in.txt",
		"dev_output_file": "data/ml/pmb220_en_org/dev_out.txt",
		"eval_input_file": "",
		"eval_output_file": "",
		"model_creation_script": [
				"global dec_tokenizer",
				"global model",
				"global criterion",
				"global optimizer",
				"global device",
				"global count_parameters",
				"global compute_loss",
				"global encoder_start_token_id",
				"global encoder_bos_token_id",
				"global decoder_start_token_id",
				"global decoder_bos_token_id", 
				"encoder_start_token_id=5",
				"encoder_bos_token_id=0",
				"decoder_start_token_id=0",
				"decoder_bos_token_id=0",
				"from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
				"encoder_config = BertConfig(vocab_size = {enc_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1)",
				"encoder = BertModel(config=encoder_config)",
				"decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1, add_cross_attention = True, is_decoder=True) ",
				"decoder = BertLMHeadModel(config=decoder_config)",
				"model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
				"print(f'The encoder has {count_parameters(encoder):,} trainable parameters')",
				"print(f'The decoder has {count_parameters(decoder):,} trainable parameters')",
				"print(f'The model has {count_parameters(model):,} trainable parameters')",
				"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
				"model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
				],
		"batch_size" : 16,
		"num_epochs" : 100,
		"min_num_epochs":80,
		"freeze_encoder_weights":"False",
		"eval_DRS_parsing_exec":"",
		"eval_Neural_DRS_exec":"",
		"model_save_path":"models/",
        "postprocess_exec": "sed -i \"s: :|||:g\" {input_file} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -f ",
		"compare_exec":"other_repos/exec_220.sh other_repos/Neural_DRS_220/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -prin",
		"check_last_num_models_to_stop":5,
		"keep_best_model":"True"
		},

	"pmb300_No-PT_8x512-8_8x512-8":{
		"comment":"This is English seq2seq model trained on pmb300 English train data",
		"do_train":"True",
		"enc_tokenizer":"pmb300_en_25000_3_2500_3_enc",
		"dec_tokenizer": "pmb300_en_25000_3_2500_3_dec",
		"train_input_file": "data/ml/pmb300_en_org/train_in.txt",
		"train_output_file": "data/ml/pmb300_en_org/train_out.txt.ser",
		"test_input_file": "data/ml/pmb300_en_org/test_in.txt",
		"test_output_file": "data/ml/pmb300_en_org/test_out.txt",
		"dev_input_file": "data/ml/pmb300_en_org/dev_in.txt",
		"dev_output_file": "data/ml/pmb300_en_org/dev_out.txt",
		"eval_input_file": "",
		"eval_output_file": "",
		"model_creation_script": [
				"global dec_tokenizer",
				"global model",
				"global criterion",
				"global optimizer",
				"global device",
				"global count_parameters",
				"global compute_loss",
				"global encoder_start_token_id",
				"global encoder_bos_token_id",
				"global decoder_start_token_id",
				"global decoder_bos_token_id", 
				"encoder_start_token_id=5",
				"encoder_bos_token_id=0",
				"decoder_start_token_id=0",
				"decoder_bos_token_id=0",
				"from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
				"encoder_config = BertConfig(vocab_size = {enc_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1)",
				"encoder = BertModel(config=encoder_config)",
				"decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1, add_cross_attention = True, is_decoder=True) ",
				"decoder = BertLMHeadModel(config=decoder_config)",
				"model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
				"print(f'The encoder has {count_parameters(encoder):,} trainable parameters')",
				"print(f'The decoder has {count_parameters(decoder):,} trainable parameters')",
				"print(f'The model has {count_parameters(model):,} trainable parameters')",
				"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
				"model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
				],
		"batch_size" : 16,
		"num_epochs" : 100,
		"min_num_epochs":80,
		"freeze_encoder_weights":"False",
		"eval_DRS_parsing_exec":"",
		"eval_Neural_DRS_exec":"",
		"model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
		"compare_exec":"other_repos/exec_300.sh other_repos/Neural_DRS_300/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -prin",
		"check_last_num_models_to_stop":5,
		"keep_best_model":"True"
		},

	"pmb400_No-PT_8x512-8_8x512-8":{
		"comment":"This is English seq2seq model trained on pmb400 English train data",
		"do_train":"True",
		"enc_tokenizer":"pmb400_en_25000_3_2500_3_enc",
		"dec_tokenizer": "pmb400_en_25000_3_2500_3_dec",
		"train_input_file": "data/ml/pmb400_en_org/train_in.txt",
		"train_output_file": "data/ml/pmb400_en_org/train_out.txt.ser",
		"test_input_file": "data/ml/pmb400_en_org/test_in.txt",
		"test_output_file": "data/ml/pmb400_en_org/test_out.txt",
		"dev_input_file": "data/ml/pmb400_en_org/dev_in.txt",
		"dev_output_file": "data/ml/pmb400_en_org/dev_out.txt",
		"eval_input_file": "data/ml/pmb400_en_org/eval_in.txt",
		"eval_output_file": "data/ml/pmb400_en_org/eval_out.txt",
		"model_creation_script": [
				"global dec_tokenizer",
				"global model",
				"global criterion",
				"global optimizer",
				"global device",
				"global count_parameters",
				"global compute_loss",
				"global encoder_start_token_id",
				"global encoder_bos_token_id",
				"global decoder_start_token_id",
				"global decoder_bos_token_id", 
				"encoder_start_token_id=5",
				"encoder_bos_token_id=0",
				"decoder_start_token_id=0",
				"decoder_bos_token_id=0",
				"from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
				"encoder_config = BertConfig(vocab_size = {enc_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1)",
				"encoder = BertModel(config=encoder_config)",
				"decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1, add_cross_attention = True, is_decoder=True) ",
				"decoder = BertLMHeadModel(config=decoder_config)",
				"model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
				"print(f'The encoder has {count_parameters(encoder):,} trainable parameters')",
				"print(f'The decoder has {count_parameters(decoder):,} trainable parameters')",
				"print(f'The model has {count_parameters(model):,} trainable parameters')",
				"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
				"model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
				],
		"batch_size" : 16,
		"num_epochs" : 100,
		"min_num_epochs":80,
		"freeze_encoder_weights":"False",
		"eval_DRS_parsing_exec":"",
		"eval_Neural_DRS_exec":"",
		"model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
		"compare_exec":"other_repos/exec_400.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
		"check_last_num_models_to_stop":5,
		"keep_best_model":"True"
		},

        "pmb220_bert_base_uncased_8x512-8":{
                "comment":"This is English seq2seq model trained on pmb220 English train data, using warm-up state of bert_base_uncased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_uncased",
                "dec_tokenizer": "pmb220_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb220_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb220_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb220_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb220_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb220_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb220_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
				"encoder =  BertModel.from_pretrained('bert-base-uncased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
				"decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "sed -i \"s: :|||:g\" {input_file} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -f ",
                "compare_exec":"other_repos/exec_220.sh other_repos/Neural_DRS_220/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
                },

        "pmb300_bert_base_uncased_8x512-8":{
                "comment":"This is English seq2seq model trained on pmb300 English train data, using warm-up state of bert_base_uncased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_uncased",
                "dec_tokenizer": "pmb300_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb300_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb300_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb300_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb300_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb300_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb300_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-base-uncased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_300.sh other_repos/Neural_DRS_300/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
		},

        "pmb400_bert_base_uncased_8x512-8":{
                "comment":"This is English seq2seq2 model trained on pmb400 English train data, using warm-up state of bert_base_uncased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_uncased",
                "dec_tokenizer": "pmb400_en_25000_3_2500_3_dec",
				"train_input_file": "data/ml/pmb400_en_org/train_in.txt",
				"train_output_file": "data/ml/pmb400_en_org/train_out.txt.ser",
				"test_input_file": "data/ml/pmb400_en_org/test_in.txt",
				"test_output_file": "data/ml/pmb400_en_org/test_out.txt",
				"dev_input_file": "data/ml/pmb400_en_org/dev_in.txt",
				"dev_output_file": "data/ml/pmb400_en_org/dev_out.txt",
				"eval_input_file": "data/ml/pmb400_en_org/eval_in.txt",
				"eval_output_file": "data/ml/pmb400_en_org/eval_out.txt",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-base-uncased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
		"compare_exec":"other_repos/exec_400.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
		"check_last_num_models_to_stop":5,
		"keep_best_model":"True"
		},

        "pmb220_bert_base_cased_8x512-8":{
                "comment":"This is English seq2seq model trained on pmb220 English train data, using warm-up state of bert_base_cased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_cased",
                "dec_tokenizer": "pmb220_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb220_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb220_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb220_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb220_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb220_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb220_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
				"encoder =  BertModel.from_pretrained('bert-base-cased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
				"decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "sed -i \"s: :|||:g\" {input_file} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -f ",
                "compare_exec":"other_repos/exec_220.sh other_repos/Neural_DRS_220/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
                },

        "pmb300_bert_base_cased_8x512-8":{
                "comment":"This is English seq2seq model trained on pmb300 English train data, using warm-up state of bert_base_cased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_cased",
                "dec_tokenizer": "pmb300_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb300_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb300_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb300_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb300_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb300_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb300_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-base-cased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_300.sh other_repos/Neural_DRS_300/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
		},

        "pmb400_bert_base_cased_8x512-8":{
                "comment":"This is English seq2seq model trained on pmb400 English train data, using warm-up state of bert_base_cased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_cased",
                "dec_tokenizer": "pmb400_en_25000_3_2500_3_dec",
				"train_input_file": "data/ml/pmb400_en_org/train_in.txt",
				"train_output_file": "data/ml/pmb400_en_org/train_out.txt.ser",
				"test_input_file": "data/ml/pmb400_en_org/test_in.txt",
				"test_output_file": "data/ml/pmb400_en_org/test_out.txt",
				"dev_input_file": "data/ml/pmb400_en_org/dev_in.txt",
				"dev_output_file": "data/ml/pmb400_en_org/dev_out.txt",
				"eval_input_file": "data/ml/pmb400_en_org/eval_in.txt",
				"eval_output_file": "data/ml/pmb400_en_org/eval_out.txt",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-base-cased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 8, num_hidden_layers = 8, hidden_size = 512, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
				"compare_exec":"other_repos/exec_400.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
				"check_last_num_models_to_stop":5,
				"keep_best_model":"True"
        },

        "pmb220_No-PT_6x768-12_6x768-12":{
                "comment":"This is English seq2seq model trained on pmb220 English train data",
                "do_train":"True",
                "enc_tokenizer":"pmb220_en_25000_3_2500_3_enc",
                "dec_tokenizer": "pmb220_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb220_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb220_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb220_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb220_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb220_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb220_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=5",
                                "encoder_bos_token_id=0",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder_config = BertConfig(vocab_size = {enc_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1)",
                                "encoder = BertModel(config=encoder_config)",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True) ",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "print(f'The encoder has {count_parameters(encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "sed -i \"s: :|||:g\" {input_file} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -f ",
                "compare_exec":"other_repos/exec_220.sh other_repos/Neural_DRS_220/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
                },

        "pmb300_No-PT_6x768-12_6x768-12":{
                "comment":"This is English seq2seq model trained on pmb300 English train data",
                "do_train":"True",
                "enc_tokenizer":"pmb300_en_25000_3_2500_3_enc",
                "dec_tokenizer": "pmb300_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb300_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb300_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb300_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb300_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb300_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb300_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=5",
                                "encoder_bos_token_id=0",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder_config = BertConfig(vocab_size = {enc_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1)",
                                "encoder = BertModel(config=encoder_config)",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True) ",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "print(f'The encoder has {count_parameters(encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs": 80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_300.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_300.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
        },
        "pmb400_No-PT_6x768-12_6x768-12":{
                "comment":"This is English seq2seq model trained on pmb400 English train data",
                "do_train":"True",
                "enc_tokenizer":"pmb400_en_25000_3_2500_3_enc",
                "dec_tokenizer": "pmb400_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb400_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb400_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb400_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb400_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb400_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb400_en_org/dev_out.txt",
                "eval_input_file": "data/ml/pmb400_en_org/eval_in.txt",
                "eval_output_file": "data/ml/pmb400_en_org/eval_out.txt",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=5",
                                "encoder_bos_token_id=0",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder_config = BertConfig(vocab_size = {enc_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1)",
                                "encoder = BertModel(config=encoder_config)",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True) ",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "print(f'The encoder has {count_parameters(encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs": 80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_400.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
        },

        "pmb220_No-PT_6x768-6_6x768-6":{
                "comment":"This is English seq2seq model trained on pmb220 English train data",
                "do_train":"True",
                "enc_tokenizer":"pmb220_en_25000_3_2500_3_enc",
                "dec_tokenizer": "pmb220_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb220_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb220_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb220_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb220_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb220_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb220_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=5",
                                "encoder_bos_token_id=0",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder_config = BertConfig(vocab_size = {enc_vocabsize}, max_position_embeddings = 512, num_attention_heads = 6, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1)",
                                "encoder = BertModel(config=encoder_config)",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 6, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True) ",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "print(f'The encoder has {count_parameters(encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "sed -i \"s: :|||:g\" {input_file} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -f ",
                "compare_exec":"other_repos/exec_220.sh other_repos/Neural_DRS_220/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
                },

        "pmb300_No-PT_6x768-6_6x768-6":{
                "comment":"This is English seq2seq model trained on pmb300 English train data",
                "do_train":"True",
                "enc_tokenizer":"pmb300_en_25000_3_2500_3_enc",
                "dec_tokenizer": "pmb300_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb300_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb300_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb300_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb300_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb300_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb300_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=5",
                                "encoder_bos_token_id=0",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder_config = BertConfig(vocab_size = {enc_vocabsize}, max_position_embeddings = 512, num_attention_heads = 6, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1)",
                                "encoder = BertModel(config=encoder_config)",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 6, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True) ",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "print(f'The encoder has {count_parameters(encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs": 80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_300.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_300.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"           
        },

        "pmb400_No-PT_6x768-6_6x768-6":{
                "comment":"This is English seq2seq model trained on pmb400 English train data",
                "do_train":"True",
                "enc_tokenizer":"pmb400_en_25000_3_2500_3_enc",
                "dec_tokenizer": "pmb400_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb400_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb400_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb400_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb400_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb400_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb400_en_org/dev_out.txt",
                "eval_input_file": "data/ml/pmb400_en_org/eval_in.txt",
                "eval_output_file": "data/ml/pmb400_en_org/eval_out.txt",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=5",
                                "encoder_bos_token_id=0",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder_config = BertConfig(vocab_size = {enc_vocabsize}, max_position_embeddings = 512, num_attention_heads = 6, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1)",
                                "encoder = BertModel(config=encoder_config)",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 6, num_hidden_layers = 6, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True) ",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "print(f'The encoder has {count_parameters(encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs": 80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_400.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
        },

        "pmb220_bert_base_uncased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb220 English train data, using warm-up state of bert_base_uncased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_uncased",
                "dec_tokenizer": "pmb220_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb220_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb220_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb220_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb220_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb220_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb220_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
				"encoder =  BertModel.from_pretrained('bert-base-uncased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "sed -i \"s: :|||:g\" {input_file} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -f ",
                "compare_exec":"other_repos/exec_220.sh other_repos/Neural_DRS_220/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
                },

        "pmb300_bert_base_uncased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb300 English train data, using warm-up state of bert_base_uncased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_uncased",
                "dec_tokenizer": "pmb300_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb300_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb300_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb300_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb300_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb300_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb300_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-base-uncased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_300.sh other_repos/Neural_DRS_300/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
		},

        "pmb400_bert_base_uncased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb400 English train data, using warm-up state of bert_base_uncased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_uncased",
                "dec_tokenizer": "pmb400_en_25000_3_2500_3_dec",
				"train_input_file": "data/ml/pmb400_en_org/train_in.txt",
				"train_output_file": "data/ml/pmb400_en_org/train_out.txt.ser",
				"test_input_file": "data/ml/pmb400_en_org/test_in.txt",
				"test_output_file": "data/ml/pmb400_en_org/test_out.txt",
				"dev_input_file": "data/ml/pmb400_en_org/dev_in.txt",
				"dev_output_file": "data/ml/pmb400_en_org/dev_out.txt",
				"eval_input_file": "data/ml/pmb400_en_org/eval_in.txt",
				"eval_output_file": "data/ml/pmb400_en_org/eval_out.txt",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-base-uncased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
		"compare_exec":"other_repos/exec_400.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
		"check_last_num_models_to_stop":5,
		"keep_best_model":"True"
		},

        "pmb220_bert_base_cased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb220 English train data, using warm-up state of bert_base_cased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_cased",
                "dec_tokenizer": "pmb220_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb220_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb220_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb220_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb220_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb220_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb220_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
				"encoder =  BertModel.from_pretrained('bert-base-cased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "sed -i \"s: :|||:g\" {input_file} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -f ",
                "compare_exec":"other_repos/exec_220.sh other_repos/Neural_DRS_220/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
                },

        "pmb300_bert_base_cased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb300 English train data, using warm-up state of bert_base_cased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_cased",
                "dec_tokenizer": "pmb300_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb300_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb300_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb300_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb300_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb300_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb300_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-base-cased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_300.sh other_repos/Neural_DRS_300/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
		},

        "pmb400_bert_base_cased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb400 English train data, using warm-up state of bert_base_cased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_cased",
                "dec_tokenizer": "pmb400_en_25000_3_2500_3_dec",
				"train_input_file": "data/ml/pmb400_en_org/train_in.txt",
				"train_output_file": "data/ml/pmb400_en_org/train_out.txt.ser",
				"test_input_file": "data/ml/pmb400_en_org/test_in.txt",
				"test_output_file": "data/ml/pmb400_en_org/test_out.txt",
				"dev_input_file": "data/ml/pmb400_en_org/dev_in.txt",
				"dev_output_file": "data/ml/pmb400_en_org/dev_out.txt",
				"eval_input_file": "data/ml/pmb400_en_org/eval_in.txt",
				"eval_output_file": "data/ml/pmb400_en_org/eval_out.txt",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-base-cased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
		"compare_exec":"other_repos/exec_400.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
		"check_last_num_models_to_stop":5,
		"keep_best_model":"True"
		},
        "pmb220_bert_large_uncased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb220 English train data, using warm-up state of bert_base_uncased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_large_uncased",
                "dec_tokenizer": "pmb220_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb220_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb220_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb220_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb220_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb220_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb220_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
				"encoder =  BertModel.from_pretrained('bert-large-uncased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "sed -i \"s: :|||:g\" {input_file} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -f ",
                "compare_exec":"other_repos/exec_220.sh other_repos/Neural_DRS_220/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
                },

        "pmb300_bert_large_uncased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb300 English train data, using warm-up state of bert_base_uncased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_large_uncased",
                "dec_tokenizer": "pmb300_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb300_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb300_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb300_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb300_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb300_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb300_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-large-uncased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_300.sh other_repos/Neural_DRS_300/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
		},

        "pmb400_bert_large_uncased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb400 English train data, using warm-up state of bert_base_uncased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_large_uncased",
                "dec_tokenizer": "pmb400_en_25000_3_2500_3_dec",
				"train_input_file": "data/ml/pmb400_en_org/train_in.txt",
				"train_output_file": "data/ml/pmb400_en_org/train_out.txt.ser",
				"test_input_file": "data/ml/pmb400_en_org/test_in.txt",
				"test_output_file": "data/ml/pmb400_en_org/test_out.txt",
				"dev_input_file": "data/ml/pmb400_en_org/dev_in.txt",
				"dev_output_file": "data/ml/pmb400_en_org/dev_out.txt",
				"eval_input_file": "data/ml/pmb400_en_org/eval_in.txt",
				"eval_output_file": "data/ml/pmb400_en_org/eval_out.txt",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-large-uncased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
		"compare_exec":"other_repos/exec_400.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
		"check_last_num_models_to_stop":5,
		"keep_best_model":"True"
		},

        "pmb220_bert_large_cased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb220 English train data, using warm-up state of bert_base_cased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_base_cased",
                "dec_tokenizer": "pmb220_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb220_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb220_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb220_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb220_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb220_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb220_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-large-cased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "sed -i \"s: :|||:g\" {input_file} && other_repos/exec_220.sh python3 other_repos/Neural_DRS_220/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -f ",
                "compare_exec":"other_repos/exec_220.sh other_repos/Neural_DRS_220/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_220/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
                },

        "pmb300_bert_large_cased_12x768-12":{
                "comment":"This is English seq2seq model trained on pmb300 English train data, using warm-up state of bert_base_cased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_large_cased",
                "dec_tokenizer": "pmb300_en_25000_3_2500_3_dec",
                "train_input_file": "data/ml/pmb300_en_org/train_in.txt",
                "train_output_file": "data/ml/pmb300_en_org/train_out.txt.ser",
                "test_input_file": "data/ml/pmb300_en_org/test_in.txt",
                "test_output_file": "data/ml/pmb300_en_org/test_out.txt",
                "dev_input_file": "data/ml/pmb300_en_org/dev_in.txt",
                "dev_output_file": "data/ml/pmb300_en_org/dev_out.txt",
                "eval_input_file": "",
                "eval_output_file": "",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-large-cased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_300.sh python3 other_repos/Neural_DRS_300/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
                "compare_exec":"other_repos/exec_300.sh other_repos/Neural_DRS_300/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_300/DRS_parsing/evaluation/clf_signature.yaml -prin",
                "check_last_num_models_to_stop":5,
                "keep_best_model":"True"
		},

        "pmb400_bert_large_cased_12x768-12":{
                "comment":"This is English seq2seq2 model trained on pmb400 English train data, using warm-up state of bert_base_cased that is frozen.",
                "do_train":"True",
                "enc_tokenizer":"bert_large_cased",
                "dec_tokenizer": "pmb400_en_25000_3_2500_3_dec",
				"train_input_file": "data/ml/pmb400_en_org/train_in.txt",
				"train_output_file": "data/ml/pmb400_en_org/train_out.txt.ser",
				"test_input_file": "data/ml/pmb400_en_org/test_in.txt",
				"test_output_file": "data/ml/pmb400_en_org/test_out.txt",
				"dev_input_file": "data/ml/pmb400_en_org/dev_in.txt",
				"dev_output_file": "data/ml/pmb400_en_org/dev_out.txt",
				"eval_input_file": "data/ml/pmb400_en_org/eval_in.txt",
				"eval_output_file": "data/ml/pmb400_en_org/eval_out.txt",
                "model_creation_script": [
                                "global dec_tokenizer",
                                "global model",
                                "global criterion",
                                "global optimizer",
                                "global device",
                                "global count_parameters",
                                "global compute_loss",
                                "global encoder_start_token_id",
                                "global encoder_bos_token_id",
                                "global decoder_start_token_id",
                                "global decoder_bos_token_id",
                                "encoder_start_token_id=101",
                                "encoder_bos_token_id=102",
                                "decoder_start_token_id=0",
                                "decoder_bos_token_id=0",
                                "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel",
                                "encoder =  BertModel.from_pretrained('bert-large-cased')",
                                "decoder_config = BertConfig(vocab_size = {dec_vocabsize}, max_position_embeddings = 512, num_attention_heads = 12, num_hidden_layers = 12, hidden_size = 768, type_vocab_size = 1, add_cross_attention = True, is_decoder=True,gradient_checkpointing=False )",
                                "decoder = BertLMHeadModel(config=decoder_config)",
                                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
                                "for param in model.encoder.parameters():",
                                "    param.requires_grad = False",
                                "print(f'The encoder has {count_parameters(model.encoder):,} trainable parameters')",
                                "print(f'The decoder has {count_parameters(model.decoder):,} trainable parameters')",
                                "print(f'The model has {count_parameters(model):,} trainable parameters')",
                                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
                                "model.to(device)",
                                "optimizer = optim.Adam(model.parameters(), lr=0.0001)",
                                "criterion = nn.NLLLoss(ignore_index=dec_tokenizer.pad_token_id)"
                                ],
                "batch_size" : 16,
                "num_epochs" : 100,
                "min_num_epochs":80,
                "freeze_encoder_weights":"False",
                "eval_DRS_parsing_exec":"",
                "eval_Neural_DRS_exec":"",
                "model_save_path":"models/",
                "postprocess_exec": "other_repos/exec_400.sh python3 other_repos/Neural_DRS_400/src/postprocess.py -i {input_file} -o {output_file} -s other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -f --no_sep",
		"compare_exec":"other_repos/exec_400.sh other_repos/Neural_DRS_400/DRS_parsing/evaluation/counter.py -f1 {test_file} -f2 {gold_file} -g other_repos/Neural_DRS_400/DRS_parsing/evaluation/clf_signature.yaml -prin",
		"check_last_num_models_to_stop":5,
		"keep_best_model":"True"
		}
	}
}
